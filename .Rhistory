model1_err_rate <- sapply(fold_ids, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(fold_ids, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(fold_ids)) * m / sqrt(S2)
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2])
print("The two classifiers have a similar error rate.")
else
print("The two classifiers do not have the same error rate at our significance level.")
}
paired_t_test_cv(output_classifier1="modelout_sepal"
,output_classifier2="modelout_petal"
,significance_level=0.05)
library(rpart)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- rpart(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRook <- rpart(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRook, newdata=testingData, type="class")
}
significance_level = 0.05
fold_ids <- unique(mydata[, "elem"])
model1_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
model2_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
model1_err_rate <- sapply(fold_ids, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(fold_ids, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(fold_ids)) * m / sqrt(S2)
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRook <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRook, newdata=testingData, type="class")
}
significance_level = 0.05
fold_ids <- unique(mydata[, "elem"])
model1_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
model2_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
model1_err_rate <- sapply(fold_ids, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(fold_ids, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(fold_ids)) * m / sqrt(S2)
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
source('~/git workspace/hypothesis testing/hypothesisTesting.R')
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRook <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRook, newdata=testingData, type="class")
}
significance_level = 0.10
fold_ids <- unique(mydata[, "elem"])
model1_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
model2_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
model1_err_rate <- sapply(fold_ids, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(fold_ids, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(fold_ids)) * m / sqrt(S2)
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRook <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRook, newdata=testingData, type="class")
}
significance_level = 0.10
kFoldLevels <- unique(mydata[, "elem"])
model1_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
model2_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
model1_err_rate <- sapply(kFoldLevels, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(kFoldLevels, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(kFoldLevels)) * m / sqrt(S2)
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRook <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRook, newdata=testingData, type="class")
}
significance_level = 0.10
kFoldLevels <- unique(mydata[, "elem"])
kFoldLevels
model1_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
model2_errors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
model1_err_rate <- sapply(kFoldLevels, function(elem) sum(model1_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
model2_err_rate <- sapply(kFoldLevels, function(elem) sum(model2_errors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(model1_err_rate - model2_err_rate)
S2 <- var(model1_err_rate - model2_err_rate)
tstat <- sqrt(length(kFoldLevels)) * m / sqrt(S2)
tstat
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(fold_ids) - 1 )
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(kFoldLevels) - 1 )
print(paste("t-statistic", tstat, sep = "="))
print(paste("Interval for", significance_level, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tstat && tstat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(length(kFoldLevels))
tTest <- (rootK * m)/ S
tstat_int <- qt(c( significance_level / 2, 1 - significance_level / 2 ), df = length(kFoldLevels) - 1 )
print(paste("t-statistic", tTest, sep = "="))
print(paste("Interval for", decisonBoundaryProb, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tTest && tTest <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +WhiteRookfile +WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile + WhiteKingrankRow +BlackKingfile +BlackKingrank, data=trainingData)
mydata[elem == mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem == mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTest <- (rootK * m)/ S
tstat_int <- qt(c( decisonBoundaryProb / 2, 1 - decisonBoundaryProb / 2 ), df = k - 1 )
print(paste("t-statistic", tTest, sep = "="))
print(paste("Interval for", decisonBoundaryProb, "significance level:"))
print(tstat_int)
if (tstat_int[1] <= tTest && tTest <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tstat_int <- qt(probVector, df = degreeOfFreedom )
print("t       Interval           ")
print(paste(tstat_int,"    ",tstat_int))
print(paste("t-statistic = ", tTestStat))
print(paste("Interval for", decisonBoundaryProb, "significance level:", tstat_int))
if (tstat_int[1] <= tTestStat && tTestStat <= tstat_int[2]) {
print("The two classifiers have a similar error rate.")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
print("t       Interval           ")
print(paste(tTestStat,"    ",tstat_int))
print(paste("t value = ", tTestStat))
print(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tstat_int[1],tstat_int[2]))
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tStatDecisonBoundary <- qt(probVector, df = degreeOfFreedom )
printResults <- function(tTestStat, tStatDecisonBoundary)
{
print(paste("t value = ", tTestStat))
print(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
print("Hypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
print("")
} else {
print("The two classifiers do not have the same error rate at our significance level.")
}
}
printResults(tTestStat, tStatDecisonBoundary)
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tStatDecisonBoundary <- qt(probVector, df = degreeOfFreedom )
printResults <- function(tTestStat, tStatDecisonBoundary)
{
print(paste("t value = ", tTestStat))
print(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
print("Hypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
print("We fail to reject the above mentioned hypothese")
} else {
print("\n\n\nWe can reject the hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tStatDecisonBoundary <- qt(probVector, df = degreeOfFreedom )
printResults <- function(tTestStat, tStatDecisonBoundary)
{
print(paste("t value = ", tTestStat))
print(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
print("Hypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\n\n\nWe can reject the hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
cat(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
printResults <- function(tTestStat, tStatDecisonBoundary)
{
cat(paste("t value = ", tTestStat))
cat(paste("Interval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
cat("Hypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\nWe can reject the hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
cat("\nHypotheses : Both classifiers perform with similar error rates")
printResults <- function(tTestStat, tStatDecisonBoundary)
{
cat(paste("\nt value = ", tTestStat))
cat(paste("\nInterval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
cat("\nHypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\nWe can reject the hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
printResults <- function(tTestStat, tStatDecisonBoundary)
{
cat(paste("\nt value = ", tTestStat))
cat(paste("\nInterval for given ",decisonBoundaryProb,"significance limit is ",tStatDecisonBoundary[1],tStatDecisonBoundary[2]))
cat("\n\nHypotheses : Both classifiers perform with similar error rates")
if((tTestStat >= tStatDecisonBoundary[1])&&(tTestStat<=tStatDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\nWe can reject the hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tStatDecisonBoundary <- qt(probVector, df = degreeOfFreedom )
printResults <- function(tValue, tDecisonBoundary)
{
cat(paste("\nt value = ", tValue))
cat(paste("\nInterval for given ",decisonBoundaryProb,"significance limit is ",tDecisonBoundary[1],tDecisonBoundary[2]))
cat("\n\nHypotheses : Both classifiers perform with similar error rates")
if((tValue >= tDecisonBoundary[1])&&(tValue<=tDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\nWe can reject the above mentioned hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
install.packages(e1071)
install.packages("e1071")
install.packages("e1071")
#install.packages("e1071")
library(e1071)
mydata <- read.table("krkopt.csv", header = TRUE, sep=",")
k <- 10
degreeOfFreedom <- k-1
kSamples <- sample(rep(1:k, ceiling(nrow(mydata)/k)))[1:nrow(mydata)]
mydata$elem <- kSamples
for (elem in 1:k){
trainingData <- mydata[elem != mydata$elem,]
testingData <- mydata[elem == mydata$elem,]
withoutBlackKingFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+WhiteRookfile+WhiteRookrank, data=trainingData)
withoutWhiteRookFit <- naiveBayes(optimalDepthOfWin ~ WhiteKingFile+WhiteKingrankRow+BlackKingfile+BlackKingrank, data=trainingData)
mydata[elem==mydata$elem, "outwithoutBlackKing"] = predict(withoutBlackKingFit, newdata=testingData, type="class")
mydata[elem==mydata$elem, "outWithoutWhiteRook"] = predict(withoutWhiteRookFit, newdata=testingData, type="class")
}
decisonBoundaryProb = 0.10
kFoldLevels <- unique(mydata[, "elem"])
withoutBlackKingModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outwithoutBlackKing"]
withoutBlackKingModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutBlackKingModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
withoutWhiteRookModelErrors <- mydata[, "optimalDepthOfWin"] != mydata[, "outWithoutWhiteRook"]
withoutWhiteRookModelAggError <- sapply(kFoldLevels, function(elem) sum(withoutWhiteRookModelErrors[mydata[, "elem"] == elem])/sum(mydata[, "elem"] == elem))
m <- mean(withoutBlackKingModelAggError - withoutWhiteRookModelAggError)
S <- sqrt(var(withoutBlackKingModelAggError - withoutWhiteRookModelAggError))
rootK <- sqrt(k)
tTestStat <- (rootK * m)/ S
probVector <- c(decisonBoundaryProb/2, 1-decisonBoundaryProb/2)
tStatDecisonBoundary <- qt(probVector, df = degreeOfFreedom )
printResults <- function(tValue, tDecisonBoundary)
{
cat(paste("\nt value = ", tValue))
cat(paste("\nInterval for given ",decisonBoundaryProb,"significance limit is ",tDecisonBoundary[1],tDecisonBoundary[2]))
cat("\n\nHypotheses : Both classifiers perform with similar error rates")
if((tValue >= tDecisonBoundary[1])&&(tValue<=tDecisonBoundary[2])){
cat("We fail to reject the above mentioned hypothese")
} else {
cat("\nWe can reject the above mentioned hypotheses")
}
}
printResults(tTestStat, tStatDecisonBoundary)
